<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 7 Machine Learning using Python | Introduction to Data Science using Python</title>
  <meta name="description" content="Chapter 7 Machine Learning using Python | Introduction to Data Science using Python" />
  <meta name="generator" content="bookdown 0.18 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 7 Machine Learning using Python | Introduction to Data Science using Python" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="araastat/BIOF085" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 7 Machine Learning using Python | Introduction to Data Science using Python" />
  
  
  

<meta name="author" content="Abhijit Dasgupta, Ph.D." />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="statistical-analysis.html"/>
<link rel="next" href="string-manipulation.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    background-color: #ffffff;
    color: #a0a0a0;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #a0a0a0;  padding-left: 4px; }
div.sourceCode
  { color: #1f1c1b; background-color: #ffffff; }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span. { color: #1f1c1b; } /* Normal */
code span.al { color: #bf0303; background-color: #f7e6e6; font-weight: bold; } /* Alert */
code span.an { color: #ca60ca; } /* Annotation */
code span.at { color: #0057ae; } /* Attribute */
code span.bn { color: #b08000; } /* BaseN */
code span.bu { color: #644a9b; font-weight: bold; } /* BuiltIn */
code span.cf { color: #1f1c1b; font-weight: bold; } /* ControlFlow */
code span.ch { color: #924c9d; } /* Char */
code span.cn { color: #aa5500; } /* Constant */
code span.co { color: #898887; } /* Comment */
code span.cv { color: #0095ff; } /* CommentVar */
code span.do { color: #607880; } /* Documentation */
code span.dt { color: #0057ae; } /* DataType */
code span.dv { color: #b08000; } /* DecVal */
code span.er { color: #bf0303; text-decoration: underline; } /* Error */
code span.ex { color: #0095ff; font-weight: bold; } /* Extension */
code span.fl { color: #b08000; } /* Float */
code span.fu { color: #644a9b; } /* Function */
code span.im { color: #ff5500; } /* Import */
code span.in { color: #b08000; } /* Information */
code span.kw { color: #1f1c1b; font-weight: bold; } /* Keyword */
code span.op { color: #1f1c1b; } /* Operator */
code span.ot { color: #006e28; } /* Other */
code span.pp { color: #006e28; } /* Preprocessor */
code span.re { color: #0057ae; background-color: #e0e9f8; } /* RegionMarker */
code span.sc { color: #3daee9; } /* SpecialChar */
code span.ss { color: #ff5500; } /* SpecialString */
code span.st { color: #bf0303; } /* String */
code span.va { color: #0057ae; } /* Variable */
code span.vs { color: #bf0303; } /* VerbatimString */
code span.wa { color: #bf0303; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
<link rel="stylesheet" href="font-awesome.min.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./index.html">BIOF 085 Manual</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> bibliography: [book.bib, packages.bib]</a></li>
<li class="chapter" data-level="2" data-path="a-python-primer.html"><a href="a-python-primer.html"><i class="fa fa-check"></i><b>2</b> A Python Primer</a><ul>
<li class="chapter" data-level="2.1" data-path="a-python-primer.html"><a href="a-python-primer.html#introduction"><i class="fa fa-check"></i><b>2.1</b> Introduction</a><ul>
<li class="chapter" data-level="2.1.1" data-path="a-python-primer.html"><a href="a-python-primer.html#python-is-a-modular-language"><i class="fa fa-check"></i><b>2.1.1</b> Python is a modular language</a></li>
<li class="chapter" data-level="2.1.2" data-path="a-python-primer.html"><a href="a-python-primer.html#python-is-a-scripting-language"><i class="fa fa-check"></i><b>2.1.2</b> Python is a scripting language</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="a-python-primer.html"><a href="a-python-primer.html#an-example"><i class="fa fa-check"></i><b>2.2</b> An example</a><ul>
<li class="chapter" data-level="2.2.1" data-path="a-python-primer.html"><a href="a-python-primer.html#some-general-rules-on-python-syntax"><i class="fa fa-check"></i><b>2.2.1</b> Some general rules on Python syntax</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="a-python-primer.html"><a href="a-python-primer.html#data-types-in-python"><i class="fa fa-check"></i><b>2.3</b> Data types in Python</a><ul>
<li class="chapter" data-level="2.3.1" data-path="a-python-primer.html"><a href="a-python-primer.html#numeric-variables"><i class="fa fa-check"></i><b>2.3.1</b> Numeric variables</a></li>
<li class="chapter" data-level="2.3.2" data-path="a-python-primer.html"><a href="a-python-primer.html#strings"><i class="fa fa-check"></i><b>2.3.2</b> Strings</a></li>
<li class="chapter" data-level="2.3.3" data-path="a-python-primer.html"><a href="a-python-primer.html#truthiness"><i class="fa fa-check"></i><b>2.3.3</b> Truthiness</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="a-python-primer.html"><a href="a-python-primer.html#data-structures-in-python"><i class="fa fa-check"></i><b>2.4</b> Data structures in Python</a><ul>
<li class="chapter" data-level="2.4.1" data-path="a-python-primer.html"><a href="a-python-primer.html#lists"><i class="fa fa-check"></i><b>2.4.1</b> Lists</a></li>
<li class="chapter" data-level="2.4.2" data-path="a-python-primer.html"><a href="a-python-primer.html#tuples"><i class="fa fa-check"></i><b>2.4.2</b> Tuples</a></li>
<li class="chapter" data-level="2.4.3" data-path="a-python-primer.html"><a href="a-python-primer.html#dictionaries"><i class="fa fa-check"></i><b>2.4.3</b> Dictionaries</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="a-python-primer.html"><a href="a-python-primer.html#operational-structures-in-python"><i class="fa fa-check"></i><b>2.5</b> Operational structures in Python</a><ul>
<li class="chapter" data-level="2.5.1" data-path="a-python-primer.html"><a href="a-python-primer.html#loops-and-list-comprehensions"><i class="fa fa-check"></i><b>2.5.1</b> Loops and list comprehensions</a></li>
<li class="chapter" data-level="2.5.2" data-path="a-python-primer.html"><a href="a-python-primer.html#list-comprehensions"><i class="fa fa-check"></i><b>2.5.2</b> List comprehensions</a></li>
<li class="chapter" data-level="2.5.3" data-path="a-python-primer.html"><a href="a-python-primer.html#conditional-evaluations"><i class="fa fa-check"></i><b>2.5.3</b> Conditional evaluations</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="a-python-primer.html"><a href="a-python-primer.html#functions"><i class="fa fa-check"></i><b>2.6</b> Functions</a><ul>
<li class="chapter" data-level="2.6.1" data-path="a-python-primer.html"><a href="a-python-primer.html#documenting-your-functions"><i class="fa fa-check"></i><b>2.6.1</b> Documenting your functions</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="a-python-primer.html"><a href="a-python-primer.html#modules-and-packages"><i class="fa fa-check"></i><b>2.7</b> Modules and Packages</a><ul>
<li class="chapter" data-level="2.7.1" data-path="a-python-primer.html"><a href="a-python-primer.html#using-modules"><i class="fa fa-check"></i><b>2.7.1</b> Using modules</a></li>
<li class="chapter" data-level="2.7.2" data-path="a-python-primer.html"><a href="a-python-primer.html#useful-modules-in-pythons-standard-library"><i class="fa fa-check"></i><b>2.7.2</b> Useful modules in Python’s standard library</a></li>
<li class="chapter" data-level="2.7.3" data-path="a-python-primer.html"><a href="a-python-primer.html#installing-third-party-packageslibraries"><i class="fa fa-check"></i><b>2.7.3</b> Installing third-party packages/libraries</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="a-python-primer.html"><a href="a-python-primer.html#environments"><i class="fa fa-check"></i><b>2.8</b> Environments</a><ul>
<li class="chapter" data-level="2.8.1" data-path="a-python-primer.html"><a href="a-python-primer.html#command-lineshell"><i class="fa fa-check"></i><b>2.8.1</b> Command-line/shell</a></li>
<li class="chapter" data-level="2.8.2" data-path="a-python-primer.html"><a href="a-python-primer.html#using-anaconda-navigator"><i class="fa fa-check"></i><b>2.8.2</b> Using Anaconda Navigator</a></li>
<li class="chapter" data-level="2.8.3" data-path="a-python-primer.html"><a href="a-python-primer.html#reproducing-environments"><i class="fa fa-check"></i><b>2.8.3</b> Reproducing environments</a></li>
</ul></li>
<li class="chapter" data-level="2.9" data-path="a-python-primer.html"><a href="a-python-primer.html#seeking-help"><i class="fa fa-check"></i><b>2.9</b> Seeking help</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="python-tools-for-data-science.html"><a href="python-tools-for-data-science.html"><i class="fa fa-check"></i><b>3</b> Python tools for data science</a><ul>
<li class="chapter" data-level="3.1" data-path="python-tools-for-data-science.html"><a href="python-tools-for-data-science.html#the-pydata-stack"><i class="fa fa-check"></i><b>3.1</b> The PyData Stack</a></li>
<li class="chapter" data-level="3.2" data-path="python-tools-for-data-science.html"><a href="python-tools-for-data-science.html#numpy-numerical-and-scientific-computing"><i class="fa fa-check"></i><b>3.2</b> Numpy (numerical and scientific computing)</a><ul>
<li class="chapter" data-level="3.2.1" data-path="python-tools-for-data-science.html"><a href="python-tools-for-data-science.html#numpy-data-types"><i class="fa fa-check"></i><b>3.2.1</b> Numpy data types</a></li>
<li class="chapter" data-level="3.2.2" data-path="python-tools-for-data-science.html"><a href="python-tools-for-data-science.html#generating-data-in-numpy"><i class="fa fa-check"></i><b>3.2.2</b> Generating data in numpy</a></li>
<li class="chapter" data-level="3.2.3" data-path="python-tools-for-data-science.html"><a href="python-tools-for-data-science.html#vectors-and-matrices"><i class="fa fa-check"></i><b>3.2.3</b> Vectors and matrices</a></li>
<li class="chapter" data-level="3.2.4" data-path="python-tools-for-data-science.html"><a href="python-tools-for-data-science.html#beware-of-copies"><i class="fa fa-check"></i><b>3.2.4</b> Beware of copies</a></li>
<li class="chapter" data-level="3.2.5" data-path="python-tools-for-data-science.html"><a href="python-tools-for-data-science.html#broadcasting-in-python"><i class="fa fa-check"></i><b>3.2.5</b> Broadcasting in Python</a></li>
<li class="chapter" data-level="3.2.6" data-path="python-tools-for-data-science.html"><a href="python-tools-for-data-science.html#conclusions-moving-forward"><i class="fa fa-check"></i><b>3.2.6</b> Conclusions moving forward</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="pandas.html"><a href="pandas.html"><i class="fa fa-check"></i><b>4</b> Pandas</a><ul>
<li class="chapter" data-level="4.1" data-path="pandas.html"><a href="pandas.html#introduction-1"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="pandas.html"><a href="pandas.html#starting-pandas"><i class="fa fa-check"></i><b>4.2</b> Starting pandas</a></li>
<li class="chapter" data-level="4.3" data-path="pandas.html"><a href="pandas.html#data-import-and-export"><i class="fa fa-check"></i><b>4.3</b> Data import and export</a></li>
<li class="chapter" data-level="4.4" data-path="pandas.html"><a href="pandas.html#exploring-a-data-set"><i class="fa fa-check"></i><b>4.4</b> Exploring a data set</a></li>
<li class="chapter" data-level="4.5" data-path="pandas.html"><a href="pandas.html#data-structures-and-types"><i class="fa fa-check"></i><b>4.5</b> Data structures and types</a><ul>
<li class="chapter" data-level="4.5.1" data-path="pandas.html"><a href="pandas.html#pandas.series"><i class="fa fa-check"></i><b>4.5.1</b> pandas.Series</a></li>
<li class="chapter" data-level="4.5.2" data-path="pandas.html"><a href="pandas.html#pandas.dataframe"><i class="fa fa-check"></i><b>4.5.2</b> pandas.DataFrame</a></li>
<li class="chapter" data-level="4.5.3" data-path="pandas.html"><a href="pandas.html#categorical-data"><i class="fa fa-check"></i><b>4.5.3</b> Categorical data</a></li>
<li class="chapter" data-level="4.5.4" data-path="pandas.html"><a href="pandas.html#missing-data"><i class="fa fa-check"></i><b>4.5.4</b> Missing data</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="pandas.html"><a href="pandas.html#data-transformation"><i class="fa fa-check"></i><b>4.6</b> Data transformation</a><ul>
<li class="chapter" data-level="4.6.1" data-path="pandas.html"><a href="pandas.html#arithmetic-operations"><i class="fa fa-check"></i><b>4.6.1</b> Arithmetic operations</a></li>
<li class="chapter" data-level="4.6.2" data-path="pandas.html"><a href="pandas.html#concatenation-of-data-sets"><i class="fa fa-check"></i><b>4.6.2</b> Concatenation of data sets</a></li>
<li class="chapter" data-level="4.6.3" data-path="pandas.html"><a href="pandas.html#merging-data-sets"><i class="fa fa-check"></i><b>4.6.3</b> Merging data sets</a></li>
<li class="chapter" data-level="4.6.4" data-path="pandas.html"><a href="pandas.html#tidy-data-principles-and-reshaping-datasets"><i class="fa fa-check"></i><b>4.6.4</b> Tidy data principles and reshaping datasets</a></li>
<li class="chapter" data-level="4.6.5" data-path="pandas.html"><a href="pandas.html#melting-unpivoting-data"><i class="fa fa-check"></i><b>4.6.5</b> Melting (unpivoting) data</a></li>
<li class="chapter" data-level="4.6.6" data-path="pandas.html"><a href="pandas.html#separating-columns-containing-multiple-variables"><i class="fa fa-check"></i><b>4.6.6</b> Separating columns containing multiple variables</a></li>
<li class="chapter" data-level="4.6.7" data-path="pandas.html"><a href="pandas.html#pivotspread-datasets"><i class="fa fa-check"></i><b>4.6.7</b> Pivot/spread datasets</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="pandas.html"><a href="pandas.html#data-aggregation-and-split-apply-combine"><i class="fa fa-check"></i><b>4.7</b> Data aggregation and split-apply-combine</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="data-visualization-using-python.html"><a href="data-visualization-using-python.html"><i class="fa fa-check"></i><b>5</b> Data visualization using Python</a><ul>
<li class="chapter" data-level="5.1" data-path="data-visualization-using-python.html"><a href="data-visualization-using-python.html#introduction-2"><i class="fa fa-check"></i><b>5.1</b> Introduction</a><ul>
<li class="chapter" data-level="5.1.1" data-path="data-visualization-using-python.html"><a href="data-visualization-using-python.html#an-example-gallery"><i class="fa fa-check"></i><b>5.1.1</b> An example gallery</a></li>
<li class="chapter" data-level="5.1.2" data-path="data-visualization-using-python.html"><a href="data-visualization-using-python.html#why-visualize-data"><i class="fa fa-check"></i><b>5.1.2</b> Why visualize data?</a></li>
<li class="chapter" data-level="5.1.3" data-path="data-visualization-using-python.html"><a href="data-visualization-using-python.html#conceptual-ideas"><i class="fa fa-check"></i><b>5.1.3</b> Conceptual ideas</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="data-visualization-using-python.html"><a href="data-visualization-using-python.html#plotting-in-python"><i class="fa fa-check"></i><b>5.2</b> Plotting in Python</a><ul>
<li class="chapter" data-level="5.2.1" data-path="data-visualization-using-python.html"><a href="data-visualization-using-python.html#static-plots"><i class="fa fa-check"></i><b>5.2.1</b> Static plots</a></li>
<li class="chapter" data-level="5.2.2" data-path="data-visualization-using-python.html"><a href="data-visualization-using-python.html#dynamic-or-interactive-plots"><i class="fa fa-check"></i><b>5.2.2</b> Dynamic or interactive plots</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="data-visualization-using-python.html"><a href="data-visualization-using-python.html#univariate-plots"><i class="fa fa-check"></i><b>5.3</b> Univariate plots</a><ul>
<li class="chapter" data-level="5.3.1" data-path="data-visualization-using-python.html"><a href="data-visualization-using-python.html#pandas-1"><i class="fa fa-check"></i><b>5.3.1</b> pandas</a></li>
<li class="chapter" data-level="5.3.2" data-path="data-visualization-using-python.html"><a href="data-visualization-using-python.html#seaborn"><i class="fa fa-check"></i><b>5.3.2</b> seaborn</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="data-visualization-using-python.html"><a href="data-visualization-using-python.html#bivariate-plots"><i class="fa fa-check"></i><b>5.4</b> Bivariate plots</a><ul>
<li class="chapter" data-level="5.4.1" data-path="data-visualization-using-python.html"><a href="data-visualization-using-python.html#pandas-2"><i class="fa fa-check"></i><b>5.4.1</b> pandas</a></li>
<li class="chapter" data-level="5.4.2" data-path="data-visualization-using-python.html"><a href="data-visualization-using-python.html#seaborn-1"><i class="fa fa-check"></i><b>5.4.2</b> seaborn</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="data-visualization-using-python.html"><a href="data-visualization-using-python.html#facets-and-multivariate-data"><i class="fa fa-check"></i><b>5.5</b> Facets and multivariate data</a><ul>
<li class="chapter" data-level="5.5.1" data-path="data-visualization-using-python.html"><a href="data-visualization-using-python.html#facets"><i class="fa fa-check"></i><b>5.5.1</b> Facets</a></li>
<li class="chapter" data-level="5.5.2" data-path="data-visualization-using-python.html"><a href="data-visualization-using-python.html#pairs-plots"><i class="fa fa-check"></i><b>5.5.2</b> Pairs plots</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="data-visualization-using-python.html"><a href="data-visualization-using-python.html#customizing-the-look"><i class="fa fa-check"></i><b>5.6</b> Customizing the look</a><ul>
<li class="chapter" data-level="5.6.1" data-path="data-visualization-using-python.html"><a href="data-visualization-using-python.html#themes"><i class="fa fa-check"></i><b>5.6.1</b> Themes</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="data-visualization-using-python.html"><a href="data-visualization-using-python.html#finer-control-with-matplotlib"><i class="fa fa-check"></i><b>5.7</b> Finer control with matplotlib</a><ul>
<li class="chapter" data-level="5.7.1" data-path="data-visualization-using-python.html"><a href="data-visualization-using-python.html#matlab-like-plotting"><i class="fa fa-check"></i><b>5.7.1</b> Matlab-like plotting</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="data-visualization-using-python.html"><a href="data-visualization-using-python.html#resources"><i class="fa fa-check"></i><b>5.8</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="statistical-analysis.html"><a href="statistical-analysis.html"><i class="fa fa-check"></i><b>6</b> Statistical analysis</a><ul>
<li class="chapter" data-level="6.1" data-path="statistical-analysis.html"><a href="statistical-analysis.html#introduction-3"><i class="fa fa-check"></i><b>6.1</b> Introduction</a></li>
<li class="chapter" data-level="6.2" data-path="statistical-analysis.html"><a href="statistical-analysis.html#descriptive-statistics"><i class="fa fa-check"></i><b>6.2</b> Descriptive statistics</a></li>
<li class="chapter" data-level="6.3" data-path="statistical-analysis.html"><a href="statistical-analysis.html#classical-hypothesis-testing"><i class="fa fa-check"></i><b>6.3</b> Classical hypothesis testing</a></li>
<li class="chapter" data-level="6.4" data-path="statistical-analysis.html"><a href="statistical-analysis.html#simulation-and-inference"><i class="fa fa-check"></i><b>6.4</b> Simulation and inference</a><ul>
<li class="chapter" data-level="6.4.1" data-path="statistical-analysis.html"><a href="statistical-analysis.html#simulation-and-hypothesis-testing"><i class="fa fa-check"></i><b>6.4.1</b> Simulation and hypothesis testing</a></li>
<li class="chapter" data-level="6.4.2" data-path="statistical-analysis.html"><a href="statistical-analysis.html#a-permutation-test"><i class="fa fa-check"></i><b>6.4.2</b> A permutation test</a></li>
<li class="chapter" data-level="6.4.3" data-path="statistical-analysis.html"><a href="statistical-analysis.html#testing-many-proteins"><i class="fa fa-check"></i><b>6.4.3</b> Testing many proteins</a></li>
<li class="chapter" data-level="6.4.4" data-path="statistical-analysis.html"><a href="statistical-analysis.html#getting-a-confidence-interval-using-the-bootstrap"><i class="fa fa-check"></i><b>6.4.4</b> Getting a confidence interval using the bootstrap</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="statistical-analysis.html"><a href="statistical-analysis.html#regression-analysis"><i class="fa fa-check"></i><b>6.5</b> Regression analysis</a><ul>
<li class="chapter" data-level="6.5.1" data-path="statistical-analysis.html"><a href="statistical-analysis.html#ordinary-least-squares-linear-regression"><i class="fa fa-check"></i><b>6.5.1</b> Ordinary least squares (linear) regression</a></li>
<li class="chapter" data-level="6.5.2" data-path="statistical-analysis.html"><a href="statistical-analysis.html#logistic-regression"><i class="fa fa-check"></i><b>6.5.2</b> Logistic regression</a></li>
<li class="chapter" data-level="6.5.3" data-path="statistical-analysis.html"><a href="statistical-analysis.html#survival-analysis"><i class="fa fa-check"></i><b>6.5.3</b> Survival analysis</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="machine-learning-using-python.html"><a href="machine-learning-using-python.html"><i class="fa fa-check"></i><b>7</b> Machine Learning using Python</a><ul>
<li class="chapter" data-level="7.1" data-path="machine-learning-using-python.html"><a href="machine-learning-using-python.html#scikit-learn"><i class="fa fa-check"></i><b>7.1</b> Scikit-learn</a><ul>
<li class="chapter" data-level="7.1.1" data-path="machine-learning-using-python.html"><a href="machine-learning-using-python.html#transforming-the-outcometarget"><i class="fa fa-check"></i><b>7.1.1</b> Transforming the outcome/target</a></li>
<li class="chapter" data-level="7.1.2" data-path="machine-learning-using-python.html"><a href="machine-learning-using-python.html#transforming-the-predictors"><i class="fa fa-check"></i><b>7.1.2</b> Transforming the predictors</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="machine-learning-using-python.html"><a href="machine-learning-using-python.html#supervised-learning"><i class="fa fa-check"></i><b>7.2</b> Supervised Learning</a><ul>
<li class="chapter" data-level="7.2.1" data-path="machine-learning-using-python.html"><a href="machine-learning-using-python.html#a-quick-example"><i class="fa fa-check"></i><b>7.2.1</b> A quick example</a></li>
<li class="chapter" data-level="7.2.2" data-path="machine-learning-using-python.html"><a href="machine-learning-using-python.html#a-data-analytic-example"><i class="fa fa-check"></i><b>7.2.2</b> A data analytic example</a></li>
<li class="chapter" data-level="7.2.3" data-path="machine-learning-using-python.html"><a href="machine-learning-using-python.html#visualizing-a-decision-tree"><i class="fa fa-check"></i><b>7.2.3</b> Visualizing a decision tree</a></li>
<li class="chapter" data-level="7.2.4" data-path="machine-learning-using-python.html"><a href="machine-learning-using-python.html#cross-validation"><i class="fa fa-check"></i><b>7.2.4</b> Cross-validation</a></li>
<li class="chapter" data-level="7.2.5" data-path="machine-learning-using-python.html"><a href="machine-learning-using-python.html#improving-models-through-cross-validation"><i class="fa fa-check"></i><b>7.2.5</b> Improving models through cross-validation</a></li>
<li class="chapter" data-level="7.2.6" data-path="machine-learning-using-python.html"><a href="machine-learning-using-python.html#feature-selection"><i class="fa fa-check"></i><b>7.2.6</b> Feature selection</a></li>
<li class="chapter" data-level="7.2.7" data-path="machine-learning-using-python.html"><a href="machine-learning-using-python.html#logistic-regression-1"><i class="fa fa-check"></i><b>7.2.7</b> Logistic regression</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="machine-learning-using-python.html"><a href="machine-learning-using-python.html#unsupervised-learning"><i class="fa fa-check"></i><b>7.3</b> Unsupervised learning</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="string-manipulation.html"><a href="string-manipulation.html"><i class="fa fa-check"></i><b>8</b> String manipulation</a><ul>
<li class="chapter" data-level="8.0.1" data-path="string-manipulation.html"><a href="string-manipulation.html#string-formatting"><i class="fa fa-check"></i><b>8.0.1</b> String formatting</a></li>
<li class="chapter" data-level="8.1" data-path="string-manipulation.html"><a href="string-manipulation.html#regular-expressions"><i class="fa fa-check"></i><b>8.1</b> Regular expressions</a><ul>
<li class="chapter" data-level="8.1.1" data-path="string-manipulation.html"><a href="string-manipulation.html#pattern-matching"><i class="fa fa-check"></i><b>8.1.1</b> Pattern matching</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Data Science using Python</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="machine-learning-using-python" class="section level1">
<h1><span class="header-section-number">Chapter 7</span> Machine Learning using Python</h1>
<div id="scikit-learn" class="section level2">
<h2><span class="header-section-number">7.1</span> Scikit-learn</h2>
<p>Scikit-learn (<code>sklearn</code>) is the main Python package for machine learning. It is a widely-used and well-regarded package. However, there are a couple of challenges to using it given the usual <code>pandas</code>-based data munging pipeline.</p>
<ol style="list-style-type: decimal">
<li><code>sklearn</code> requires that all inputs be numeric, and in fact, <code>numpy</code> arrays.</li>
<li><code>sklearn</code> requires that all categorical variables by replaced by 0/1 dummy variables</li>
<li><code>sklearn</code> requires us to separate the predictors from the outcome. We need to have one <code>X</code> matrix for the predictors and one <code>y</code> vector for the outcome.</li>
</ol>
<p>The big issue, of course, is the first point. Given we used <code>pandas</code> precisely because we wanted to be able to keep heterogenous data. We have to be able to convert non-numeric data to numeric. <code>pandas</code> does help us out with this problem.</p>
<ol style="list-style-type: decimal">
<li>First of all, we know that all <code>pandas</code> Series and DataFrame objects can be converted to <code>numpy</code> arrays using the <code>values</code> or <code>to_numpy</code> functions.</li>
<li>Second, we can easily extract a single variable from the data set using either the usual extracton methods or the
<code>pop</code> function.</li>
<li>Third, <code>pandas</code> gives us a way to convert all categorical values to numeric dummy variables using the <code>get_dummies</code> function. This is actually a more desirable solution than what you will see in cyberspace, which is to use the
<code>OneHotEncoder</code> function from <code>sklearn</code>.
<ul>
<li>This is generally fine since many machine learning models look for interactions internally and don’t need them to be overtly specified. The main exceptions to this are linear and logistic regression. For those, we can use the formula methods described in the Statistical Modeling module to generate the appropriately transformed design matrix.</li>
<li>If the outcome variable is not numeric, we can <code>LabelEncoder</code> function from the <code>sklearn.preprocessing</code> submodule to convert it.</li>
</ul></li>
</ol>
<p>I just threw a bunch of jargon at you. Let’s see what this means.</p>
<div id="transforming-the-outcometarget" class="section level3">
<h3><span class="header-section-number">7.1.1</span> Transforming the outcome/target</h3>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="machine-learning-using-python.html#cb2-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-2"><a href="machine-learning-using-python.html#cb2-2"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb2-3"><a href="machine-learning-using-python.html#cb2-3"></a><span class="im">import</span> sklearn</span>
<span id="cb2-4"><a href="machine-learning-using-python.html#cb2-4"></a><span class="im">import</span> statsmodels.api <span class="im">as</span> sm</span>
<span id="cb2-5"><a href="machine-learning-using-python.html#cb2-5"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb2-6"><a href="machine-learning-using-python.html#cb2-6"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb2-7"><a href="machine-learning-using-python.html#cb2-7"></a></span>
<span id="cb2-8"><a href="machine-learning-using-python.html#cb2-8"></a>iris <span class="op">=</span> sm.datasets.get_rdataset(<span class="st">&#39;iris&#39;</span>).data</span>
<span id="cb2-9"><a href="machine-learning-using-python.html#cb2-9"></a>iris.head()</span></code></pre></div>
<pre><code>##    Sepal.Length  Sepal.Width  Petal.Length  Petal.Width Species
## 0           5.1          3.5           1.4          0.2  setosa
## 1           4.9          3.0           1.4          0.2  setosa
## 2           4.7          3.2           1.3          0.2  setosa
## 3           4.6          3.1           1.5          0.2  setosa
## 4           5.0          3.6           1.4          0.2  setosa</code></pre>
<p>Let’s hit the first issue first. We need to separate out the outcome (the variable we want to predict) from the predictors (in this case the sepal and petal measurements).</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="machine-learning-using-python.html#cb4-1"></a>y <span class="op">=</span> iris[<span class="st">&#39;Species&#39;</span>]</span>
<span id="cb4-2"><a href="machine-learning-using-python.html#cb4-2"></a>X <span class="op">=</span> iris.drop(<span class="st">&#39;Species&#39;</span>, axis <span class="op">=</span> <span class="dv">1</span>) <span class="co"># drops column, makes a copy</span></span></code></pre></div>
<p>Another way to do this is</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="machine-learning-using-python.html#cb5-1"></a>y <span class="op">=</span> iris.pop(<span class="st">&#39;Species&#39;</span>)</span></code></pre></div>
<p>If you look at this, <code>iris</code> now only has 4 columns. So we could just use <code>iris</code> after the <code>pop</code> application, as the predictor set</p>
<p>We still have to update <code>y</code> to become numeric. This is where the <code>sklearn</code> functions start to be handy</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="machine-learning-using-python.html#cb6-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> LabelEncoder</span>
<span id="cb6-2"><a href="machine-learning-using-python.html#cb6-2"></a>le <span class="op">=</span> LabelEncoder()</span>
<span id="cb6-3"><a href="machine-learning-using-python.html#cb6-3"></a>y <span class="op">=</span> le.fit_transform(y)</span>
<span id="cb6-4"><a href="machine-learning-using-python.html#cb6-4"></a>y</span></code></pre></div>
<pre><code>## array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
##        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
##        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
##        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
##        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
##        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
##        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])</code></pre>
<p>Let’s talk about this code, since it’s very typical of the way the <code>sklearn</code>
code works. First, we import a method (<code>LabelEncoder</code>) from the appropriate
<code>sklearn</code> module. The second line, <code>le = LabelEncoder()</code> works to “turn on” the
method. This is like taking a power tool off the shelf and plugging it in to a
socket. It’s now ready to work. The third line does the actual work. The
<code>fit_transform</code> function transforms the data you input into it based on the
method it is then attached to.</p>
<blockquote>
<p>Let’s make a quick analogy. You can plug in both a power washer and a
jackhammer to get them ready to go. You can then apply each of them to your
driveway. They “transform” the driveway in different ways depending on which
tool is used. The washer would “transform” the driveway by cleaning it, while
the jackhammer would transform the driveway by breaking it.</p>
</blockquote>
<p>There’s an interesting invisible quirk to the code, though. The object <code>le</code> also got transformed during this
process. There were pieces added to it during the <code>fit_transform</code> process.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="machine-learning-using-python.html#cb8-1"></a>le <span class="op">=</span> LabelEncoder()</span>
<span id="cb8-2"><a href="machine-learning-using-python.html#cb8-2"></a>d1 <span class="op">=</span> <span class="bu">dir</span>(le)</span></code></pre></div>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="machine-learning-using-python.html#cb9-1"></a>y <span class="op">=</span> le.fit_transform( pd.read_csv(<span class="st">&#39;data/iris.csv&#39;</span>)[<span class="st">&#39;species&#39;</span>])</span>
<span id="cb9-2"><a href="machine-learning-using-python.html#cb9-2"></a>d2 <span class="op">=</span> <span class="bu">dir</span>(le)</span>
<span id="cb9-3"><a href="machine-learning-using-python.html#cb9-3"></a><span class="bu">set</span>(d2).difference(<span class="bu">set</span>(d1)) <span class="co"># set of things in d2 but not in d1</span></span></code></pre></div>
<pre><code>## {&#39;classes_&#39;}</code></pre>
<p>So we see that there is a new component added, called <code>classes_</code>.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="machine-learning-using-python.html#cb11-1"></a>le.classes_</span></code></pre></div>
<pre><code>## array([&#39;setosa&#39;, &#39;versicolor&#39;, &#39;virginica&#39;], dtype=object)</code></pre>
<p>So the original labels aren’t destroyed; they are being stored. This can be useful.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="machine-learning-using-python.html#cb13-1"></a>le.inverse_transform([<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">0</span>])</span></code></pre></div>
<pre><code>## array([&#39;setosa&#39;, &#39;versicolor&#39;, &#39;versicolor&#39;, &#39;virginica&#39;, &#39;setosa&#39;],
##       dtype=object)</code></pre>
<p>So we can transform back from the numeric to the labels. Keep this in hand, since it will prove useful after
we have done some predictions using a ML model, which will give numeric predictions.</p>
</div>
<div id="transforming-the-predictors" class="section level3">
<h3><span class="header-section-number">7.1.2</span> Transforming the predictors</h3>
<p>Let’s look at a second example. The <code>diamonds</code> dataset has several categorical variables that would need to be transformed.</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="machine-learning-using-python.html#cb15-1"></a>diamonds <span class="op">=</span> pd.read_csv(<span class="st">&#39;data/diamonds.csv.gz&#39;</span>)</span>
<span id="cb15-2"><a href="machine-learning-using-python.html#cb15-2"></a></span>
<span id="cb15-3"><a href="machine-learning-using-python.html#cb15-3"></a>y <span class="op">=</span> diamonds.pop(<span class="st">&#39;price&#39;</span>).values</span>
<span id="cb15-4"><a href="machine-learning-using-python.html#cb15-4"></a>X <span class="op">=</span> pd.get_dummies(diamonds)</span>
<span id="cb15-5"><a href="machine-learning-using-python.html#cb15-5"></a></span>
<span id="cb15-6"><a href="machine-learning-using-python.html#cb15-6"></a><span class="co"># Alternatively</span></span>
<span id="cb15-7"><a href="machine-learning-using-python.html#cb15-7"></a><span class="co"># import patsy</span></span>
<span id="cb15-8"><a href="machine-learning-using-python.html#cb15-8"></a><span class="co"># f = &#39;~ np.log(carat) +  + clarity + depth + cut * color&#39;</span></span>
<span id="cb15-9"><a href="machine-learning-using-python.html#cb15-9"></a><span class="co"># X = patsy.dmatrix(f, data=diamonds)</span></span></code></pre></div>
<div class="sourceCode" id="cb16"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="machine-learning-using-python.html#cb16-1"></a><span class="bu">type</span>(X)</span></code></pre></div>
<pre><code>## &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;</code></pre>
<div class="sourceCode" id="cb18"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="machine-learning-using-python.html#cb18-1"></a>X.info()</span></code></pre></div>
<pre><code>## &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
## RangeIndex: 53940 entries, 0 to 53939
## Data columns (total 26 columns):
##  #   Column         Non-Null Count  Dtype  
## ---  ------         --------------  -----  
##  0   carat          53940 non-null  float64
##  1   depth          53940 non-null  float64
##  2   table          53940 non-null  float64
##  3   x              53940 non-null  float64
##  4   y              53940 non-null  float64
##  5   z              53940 non-null  float64
##  6   cut_Fair       53940 non-null  uint8  
##  7   cut_Good       53940 non-null  uint8  
##  8   cut_Ideal      53940 non-null  uint8  
##  9   cut_Premium    53940 non-null  uint8  
##  10  cut_Very Good  53940 non-null  uint8  
##  11  color_D        53940 non-null  uint8  
##  12  color_E        53940 non-null  uint8  
##  13  color_F        53940 non-null  uint8  
##  14  color_G        53940 non-null  uint8  
##  15  color_H        53940 non-null  uint8  
##  16  color_I        53940 non-null  uint8  
##  17  color_J        53940 non-null  uint8  
##  18  clarity_I1     53940 non-null  uint8  
##  19  clarity_IF     53940 non-null  uint8  
##  20  clarity_SI1    53940 non-null  uint8  
##  21  clarity_SI2    53940 non-null  uint8  
##  22  clarity_VS1    53940 non-null  uint8  
##  23  clarity_VS2    53940 non-null  uint8  
##  24  clarity_VVS1   53940 non-null  uint8  
##  25  clarity_VVS2   53940 non-null  uint8  
## dtypes: float64(6), uint8(20)
## memory usage: 3.5 MB</code></pre>
<p>So everything is now numeric!!. Let’s take a peek inside.</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb20-1"><a href="machine-learning-using-python.html#cb20-1"></a>X.columns</span></code></pre></div>
<pre><code>## Index([&#39;carat&#39;, &#39;depth&#39;, &#39;table&#39;, &#39;x&#39;, &#39;y&#39;, &#39;z&#39;, &#39;cut_Fair&#39;, &#39;cut_Good&#39;,
##        &#39;cut_Ideal&#39;, &#39;cut_Premium&#39;, &#39;cut_Very Good&#39;, &#39;color_D&#39;, &#39;color_E&#39;,
##        &#39;color_F&#39;, &#39;color_G&#39;, &#39;color_H&#39;, &#39;color_I&#39;, &#39;color_J&#39;, &#39;clarity_I1&#39;,
##        &#39;clarity_IF&#39;, &#39;clarity_SI1&#39;, &#39;clarity_SI2&#39;, &#39;clarity_VS1&#39;,
##        &#39;clarity_VS2&#39;, &#39;clarity_VVS1&#39;, &#39;clarity_VVS2&#39;],
##       dtype=&#39;object&#39;)</code></pre>
<p>So, it looks like the continuous variables remain intact, but the categorical variables got exploded out. Each
variable name has a level with it, which represents the particular level it is representing. Each of these
variables, called dummy variables, are numerical 0/1 variables. For example, <code>color_F</code> is 1 for those diamonds which have color F, and 0 otherwise.</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb22-1"><a href="machine-learning-using-python.html#cb22-1"></a>pd.crosstab(X[<span class="st">&#39;color_F&#39;</span>], diamonds[<span class="st">&#39;color&#39;</span>])</span></code></pre></div>
<pre><code>## color       D     E     F      G     H     I     J
## color_F                                           
## 0        6775  9797     0  11292  8304  5422  2808
## 1           0     0  9542      0     0     0     0</code></pre>
</div>
</div>
<div id="supervised-learning" class="section level2">
<h2><span class="header-section-number">7.2</span> Supervised Learning</h2>
<p>We will first look at supervised learning methods.</p>
<table>
<colgroup>
<col width="27%" />
<col width="72%" />
</colgroup>
<thead>
<tr class="header">
<th>ML method</th>
<th>Code to call it</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Decision Tree</td>
<td><code>sklearn.tree.DecisionTreeClassifier</code>, <code>sklearn.tree.DecisionTreeRegressor</code></td>
</tr>
<tr class="even">
<td>Random Forest</td>
<td><code>sklearn.ensemble.RandomForestClassifier</code>, <code>sklearn.ensemble.RandomForestRegressor</code></td>
</tr>
<tr class="odd">
<td>Linear Regression</td>
<td><code>sklearn.linear_model.LinearRegression</code></td>
</tr>
<tr class="even">
<td>Logistic Regression</td>
<td><code>sklearn.linear_model.LogisticRegression</code></td>
</tr>
<tr class="odd">
<td>Support Vector Machines</td>
<td><code>sklearn.svm.LinearSVC</code>, <code>sklearn.svm.LinearSVR</code></td>
</tr>
<tr class="even">
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>The general method that the code will follow is :</p>
<pre><code>from sklearn.... import Machine
machine = Machine(*parameters*)
machine.fit(X, y)</code></pre>
<div id="a-quick-example" class="section level3">
<h3><span class="header-section-number">7.2.1</span> A quick example</h3>
<div class="sourceCode" id="cb25"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb25-1"><a href="machine-learning-using-python.html#cb25-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb25-2"><a href="machine-learning-using-python.html#cb25-2"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeRegressor</span>
<span id="cb25-3"><a href="machine-learning-using-python.html#cb25-3"></a></span>
<span id="cb25-4"><a href="machine-learning-using-python.html#cb25-4"></a>lm <span class="op">=</span> LinearRegression()</span>
<span id="cb25-5"><a href="machine-learning-using-python.html#cb25-5"></a>dt <span class="op">=</span> DecisionTreeRegressor()</span></code></pre></div>
<p>Lets manufacture some data</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb26-1"><a href="machine-learning-using-python.html#cb26-1"></a>x <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">10</span>, <span class="dv">200</span>)</span>
<span id="cb26-2"><a href="machine-learning-using-python.html#cb26-2"></a>y <span class="op">=</span> <span class="dv">2</span> <span class="op">+</span> <span class="dv">3</span><span class="op">*</span>x <span class="op">-</span> <span class="dv">5</span><span class="op">*</span>(x<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb26-3"><a href="machine-learning-using-python.html#cb26-3"></a>d <span class="op">=</span> pd.DataFrame({<span class="st">&#39;x&#39;</span>: x})</span>
<span id="cb26-4"><a href="machine-learning-using-python.html#cb26-4"></a></span>
<span id="cb26-5"><a href="machine-learning-using-python.html#cb26-5"></a>lm.fit(d,y)<span class="op">;</span></span></code></pre></div>
<pre><code>## LinearRegression()</code></pre>
<div class="sourceCode" id="cb28"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb28-1"><a href="machine-learning-using-python.html#cb28-1"></a>dt.fit(d, y)<span class="op">;</span></span></code></pre></div>
<pre><code>## DecisionTreeRegressor()</code></pre>
<div class="sourceCode" id="cb30"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb30-1"><a href="machine-learning-using-python.html#cb30-1"></a>p1 <span class="op">=</span> lm.predict(d)</span>
<span id="cb30-2"><a href="machine-learning-using-python.html#cb30-2"></a>p2 <span class="op">=</span> dt.predict(d)</span>
<span id="cb30-3"><a href="machine-learning-using-python.html#cb30-3"></a></span>
<span id="cb30-4"><a href="machine-learning-using-python.html#cb30-4"></a>d[<span class="st">&#39;y&#39;</span>] <span class="op">=</span> y</span>
<span id="cb30-5"><a href="machine-learning-using-python.html#cb30-5"></a>d[<span class="st">&#39;lm&#39;</span>] <span class="op">=</span> p1</span>
<span id="cb30-6"><a href="machine-learning-using-python.html#cb30-6"></a>d[<span class="st">&#39;dt&#39;</span>] <span class="op">=</span> p2</span>
<span id="cb30-7"><a href="machine-learning-using-python.html#cb30-7"></a></span>
<span id="cb30-8"><a href="machine-learning-using-python.html#cb30-8"></a>D <span class="op">=</span> pd.melt(d, id_vars <span class="op">=</span> <span class="st">&#39;x&#39;</span>)</span>
<span id="cb30-9"><a href="machine-learning-using-python.html#cb30-9"></a></span>
<span id="cb30-10"><a href="machine-learning-using-python.html#cb30-10"></a>sns.relplot(data<span class="op">=</span>D, x <span class="op">=</span> <span class="st">&#39;x&#39;</span>, y <span class="op">=</span> <span class="st">&#39;value&#39;</span>, hue <span class="op">=</span> <span class="st">&#39;variable&#39;</span>)<span class="op">;</span></span></code></pre></div>
<pre><code>## &lt;seaborn.axisgrid.FacetGrid object at 0x1380bfb80&gt;</code></pre>
<div class="sourceCode" id="cb32"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb32-1"><a href="machine-learning-using-python.html#cb32-1"></a>plt.show()</span></code></pre></div>
<p><img src="BIOF085_Manual_files/figure-html/05-python-learning-15-1.png" width="582" /></p>
</div>
<div id="a-data-analytic-example" class="section level3">
<h3><span class="header-section-number">7.2.2</span> A data analytic example</h3>
<div class="sourceCode" id="cb33"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb33-1"><a href="machine-learning-using-python.html#cb33-1"></a>diamonds <span class="op">=</span> pd.read_csv(<span class="st">&#39;data/diamonds.csv.gz&#39;</span>)</span>
<span id="cb33-2"><a href="machine-learning-using-python.html#cb33-2"></a>diamonds.info()</span></code></pre></div>
<pre><code>## &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
## RangeIndex: 53940 entries, 0 to 53939
## Data columns (total 10 columns):
##  #   Column   Non-Null Count  Dtype  
## ---  ------   --------------  -----  
##  0   carat    53940 non-null  float64
##  1   cut      53940 non-null  object 
##  2   color    53940 non-null  object 
##  3   clarity  53940 non-null  object 
##  4   depth    53940 non-null  float64
##  5   table    53940 non-null  float64
##  6   price    53940 non-null  int64  
##  7   x        53940 non-null  float64
##  8   y        53940 non-null  float64
##  9   z        53940 non-null  float64
## dtypes: float64(6), int64(1), object(3)
## memory usage: 4.1+ MB</code></pre>
<p>First, lets separate out the outcome (price) and the predictors</p>
<div class="sourceCode" id="cb35"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb35-1"><a href="machine-learning-using-python.html#cb35-1"></a>y <span class="op">=</span> diamonds.pop(<span class="st">&#39;price&#39;</span>)</span></code></pre></div>
<p>For many machine learning problems, it is useful to scale the numeric predictors so that they have mean 0 and
variance 1. First we need to separate out the categorical and numeric variables</p>
<div class="sourceCode" id="cb36"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb36-1"><a href="machine-learning-using-python.html#cb36-1"></a>d1 <span class="op">=</span> diamonds.select_dtypes(include <span class="op">=</span> <span class="st">&#39;number&#39;</span>)</span>
<span id="cb36-2"><a href="machine-learning-using-python.html#cb36-2"></a>d2 <span class="op">=</span> diamonds.select_dtypes(exclude <span class="op">=</span> <span class="st">&#39;number&#39;</span>)</span></code></pre></div>
<p>Now let’s scale the columns of <code>d1</code></p>
<div class="sourceCode" id="cb37"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb37-1"><a href="machine-learning-using-python.html#cb37-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> scale</span>
<span id="cb37-2"><a href="machine-learning-using-python.html#cb37-2"></a></span>
<span id="cb37-3"><a href="machine-learning-using-python.html#cb37-3"></a>bl <span class="op">=</span> scale(d1)</span>
<span id="cb37-4"><a href="machine-learning-using-python.html#cb37-4"></a>bl</span></code></pre></div>
<pre><code>## array([[-1.19816781, -0.17409151, -1.09967199, -1.58783745, -1.53619556,
##         -1.57112919],
##        [-1.24036129, -1.36073849,  1.58552871, -1.64132529, -1.65877419,
##         -1.74117497],
##        [-1.19816781, -3.38501862,  3.37566251, -1.49869105, -1.45739502,
##         -1.74117497],
##        ...,
##        [-0.20662095,  0.73334442,  1.13799526, -0.06343409, -0.04774083,
##          0.03013526],
##        [ 0.13092691, -0.52310533,  0.24292836,  0.37338325,  0.33750627,
##          0.28520393],
##        [-0.10113725,  0.31452784, -1.09967199,  0.08811478,  0.11861587,
##          0.14349912]])</code></pre>
<p>Woops!! We get a <code>numpy</code> array, not a <code>DataFrame</code>!!</p>
<div class="sourceCode" id="cb39"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb39-1"><a href="machine-learning-using-python.html#cb39-1"></a>bl <span class="op">=</span> pd.DataFrame(scale(d1))</span>
<span id="cb39-2"><a href="machine-learning-using-python.html#cb39-2"></a>bl.columns <span class="op">=</span> <span class="bu">list</span>(d1.columns)</span>
<span id="cb39-3"><a href="machine-learning-using-python.html#cb39-3"></a>d1 <span class="op">=</span> bl</span></code></pre></div>
<p>Now, let’s recode the categorical variables into dummy variables.</p>
<div class="sourceCode" id="cb40"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb40-1"><a href="machine-learning-using-python.html#cb40-1"></a>d2 <span class="op">=</span> pd.get_dummies(d2)</span></code></pre></div>
<p>and put them back together</p>
<div class="sourceCode" id="cb41"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb41-1"><a href="machine-learning-using-python.html#cb41-1"></a>X <span class="op">=</span> pd.concat([d1,d2], axis <span class="op">=</span> <span class="dv">1</span>)</span></code></pre></div>
<p>Next we need to split the data into a training set and a test set. Usually we do this as an 80/20 split.
The purpose of the test set is to see how well the model works on an “external” data set. We don’t touch the
test set until we’re done with all our model building in the training set. We usually do the split using
random numbers. We’ll put 40,000 observations in the training set.</p>
<div class="sourceCode" id="cb42"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb42-1"><a href="machine-learning-using-python.html#cb42-1"></a>ind <span class="op">=</span> <span class="bu">list</span>(X.index)</span>
<span id="cb42-2"><a href="machine-learning-using-python.html#cb42-2"></a>np.random.shuffle(ind)</span>
<span id="cb42-3"><a href="machine-learning-using-python.html#cb42-3"></a></span>
<span id="cb42-4"><a href="machine-learning-using-python.html#cb42-4"></a>X_train, y_train <span class="op">=</span> X.loc[ind[:<span class="dv">40000</span>],:], y[ind[:<span class="dv">40000</span>]]</span>
<span id="cb42-5"><a href="machine-learning-using-python.html#cb42-5"></a>X_test, y_test <span class="op">=</span> X.loc[ind[<span class="dv">40000</span>:],:], y[ind[<span class="dv">40000</span>:]]</span></code></pre></div>
<p>There is another way to do this</p>
<div class="sourceCode" id="cb43"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb43-1"><a href="machine-learning-using-python.html#cb43-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb43-2"><a href="machine-learning-using-python.html#cb43-2"></a></span>
<span id="cb43-3"><a href="machine-learning-using-python.html#cb43-3"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y , test_size <span class="op">=</span> <span class="fl">0.2</span>, random_state<span class="op">=</span> <span class="dv">40</span>)</span></code></pre></div>
<p>Now we will fit our models to the training data. Let’s use a decision tree model, a random forest model, and a linear regression.</p>
<div class="sourceCode" id="cb44"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb44-1"><a href="machine-learning-using-python.html#cb44-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb44-2"><a href="machine-learning-using-python.html#cb44-2"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeRegressor</span>
<span id="cb44-3"><a href="machine-learning-using-python.html#cb44-3"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> RandomForestRegressor</span>
<span id="cb44-4"><a href="machine-learning-using-python.html#cb44-4"></a></span>
<span id="cb44-5"><a href="machine-learning-using-python.html#cb44-5"></a>lm <span class="op">=</span> LinearRegression()<span class="op">;</span></span>
<span id="cb44-6"><a href="machine-learning-using-python.html#cb44-6"></a>dt <span class="op">=</span> DecisionTreeRegressor()<span class="op">;</span></span>
<span id="cb44-7"><a href="machine-learning-using-python.html#cb44-7"></a>rf <span class="op">=</span> RandomForestRegressor()<span class="op">;</span></span></code></pre></div>
<p>Now we will use our training data to fit the models</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb45-1"><a href="machine-learning-using-python.html#cb45-1"></a>lm.fit(X_train, y_train)<span class="op">;</span></span></code></pre></div>
<div class="sourceCode" id="cb46"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb46-1"><a href="machine-learning-using-python.html#cb46-1"></a>dt.fit(X_train, y_train)<span class="op">;</span></span></code></pre></div>
<div class="sourceCode" id="cb47"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb47-1"><a href="machine-learning-using-python.html#cb47-1"></a>rf.fit(X_train, y_train)<span class="op">;</span></span></code></pre></div>
<p>We now need to see how well the model fit the data. We’ll use the R2 statistic to be our metric of choice to evaluate the model fit.</p>
<div class="sourceCode" id="cb48"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb48-1"><a href="machine-learning-using-python.html#cb48-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span>  r2_score</span>
<span id="cb48-2"><a href="machine-learning-using-python.html#cb48-2"></a></span>
<span id="cb48-3"><a href="machine-learning-using-python.html#cb48-3"></a>pd.DataFrame({</span>
<span id="cb48-4"><a href="machine-learning-using-python.html#cb48-4"></a>  <span class="st">&#39;Model&#39;</span>: [<span class="st">&#39;Linear regression&#39;</span>,<span class="st">&#39;Decision tree&#39;</span>,<span class="st">&#39;Random forest&#39;</span>],</span>
<span id="cb48-5"><a href="machine-learning-using-python.html#cb48-5"></a>  <span class="st">&#39;R2&#39;</span>: [r2_score(y_train, lm.predict(X_train)),</span>
<span id="cb48-6"><a href="machine-learning-using-python.html#cb48-6"></a>    r2_score(y_train, dt.predict(X_train)),</span>
<span id="cb48-7"><a href="machine-learning-using-python.html#cb48-7"></a>    r2_score(y_train, rf.predict(X_train))]</span>
<span id="cb48-8"><a href="machine-learning-using-python.html#cb48-8"></a>})</span></code></pre></div>
<pre><code>##                Model        R2
## 0  Linear regression  0.920264
## 1      Decision tree  0.999997
## 2      Random forest  0.997335</code></pre>
<p>This is pretty amazing. However, we know that if we try and predict using the same data we used to train
the model, we get better than expected results. One way to get a better idea about the true performance of the
model when we will try it on external data is to do cross-validation.</p>
</div>
<div id="visualizing-a-decision-tree" class="section level3">
<h3><span class="header-section-number">7.2.3</span> Visualizing a decision tree</h3>
<p><strong>scikit-learn</strong> provides a decent way of visualizing a decision tree using a program called <em>Graphviz</em>, which is a dedicated graph and network visualization program.</p>
<div class="sourceCode" id="cb50"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb50-1"><a href="machine-learning-using-python.html#cb50-1"></a><span class="im">import</span> graphviz</span>
<span id="cb50-2"><a href="machine-learning-using-python.html#cb50-2"></a><span class="im">from</span> sklearn <span class="im">import</span> tree</span>
<span id="cb50-3"><a href="machine-learning-using-python.html#cb50-3"></a></span>
<span id="cb50-4"><a href="machine-learning-using-python.html#cb50-4"></a>dt <span class="op">=</span> DecisionTreeRegressor(max_depth<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb50-5"><a href="machine-learning-using-python.html#cb50-5"></a>dt.fit(X_train, y_train)<span class="op">;</span></span></code></pre></div>
<div class="sourceCode" id="cb51"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb51-1"><a href="machine-learning-using-python.html#cb51-1"></a>dot_data <span class="op">=</span> tree.export_graphviz(dt, out_file<span class="op">=</span><span class="va">None</span>, </span>
<span id="cb51-2"><a href="machine-learning-using-python.html#cb51-2"></a>                                feature_names <span class="op">=</span> X_train.columns,</span>
<span id="cb51-3"><a href="machine-learning-using-python.html#cb51-3"></a>                                filled<span class="op">=</span><span class="va">True</span>, rounded<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb51-4"><a href="machine-learning-using-python.html#cb51-4"></a>graph <span class="op">=</span> graphviz.Source(dot_data)<span class="op">;</span></span>
<span id="cb51-5"><a href="machine-learning-using-python.html#cb51-5"></a>graph</span></code></pre></div>
<p><img src="graphs/image.png" width="603" /></p>
</div>
<div id="cross-validation" class="section level3">
<h3><span class="header-section-number">7.2.4</span> Cross-validation</h3>
<p>In cross-validation, we split the dataset up into 5 equal parts randomly. We then train the
model using 4 parts and predict the data on the 5th part. We do for all possible groups of 4 parts. We then
consider the overall performance of prediction.</p>
<p><img src="graphs/CV5.png" /></p>
<p>There is nothing special about the 5 splits. If you use 5 splits, it is called 5-fold cross-validation (CV), if you use 10 splits, it is 10-fold CV. If you use all but one subject as training data, and that one subject as test data, and cycle through all the subjects, that is called leave-one-out CV (LOOCV). All these methods are widely used, but 5- and 10-fold CV are often used as a balance between effectiveness and computational efficiency.</p>
<p><strong>scikit-learn</strong> makes this pretty easy, using the <code>cross_val_score</code> function.</p>
<div class="sourceCode" id="cb52"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb52-1"><a href="machine-learning-using-python.html#cb52-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> cross_val_score</span>
<span id="cb52-2"><a href="machine-learning-using-python.html#cb52-2"></a>cv_score <span class="op">=</span> cross_val_score(dt, X_train, y_train, cv<span class="op">=</span><span class="dv">5</span>, scoring<span class="op">=</span><span class="st">&#39;r2&#39;</span>)</span>
<span id="cb52-3"><a href="machine-learning-using-python.html#cb52-3"></a><span class="ss">f&quot;CV error = </span><span class="sc">{np.</span><span class="bu">round</span>(np.mean(cv_score), <span class="dv">3</span>)<span class="sc">}</span><span class="ss">&quot;</span></span></code></pre></div>
<pre><code>## &#39;CV error = 0.874&#39;</code></pre>
</div>
<div id="improving-models-through-cross-validation" class="section level3">
<h3><span class="header-section-number">7.2.5</span> Improving models through cross-validation</h3>
<p>The cross-validation error, as we’ve seen, gives us a better estimate of
how well our model predicts on new data. We can use this to tune models by tweaking their parameters to get models that reasonably will perform better.</p>
<p>Each model that we fit has a set of parameters that govern how it proceeds
to fit the data. These can bee seen using the <code>get_params</code> function.</p>
<div class="sourceCode" id="cb54"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb54-1"><a href="machine-learning-using-python.html#cb54-1"></a>dt.get_params()</span></code></pre></div>
<pre><code>## {&#39;ccp_alpha&#39;: 0.0, &#39;criterion&#39;: &#39;mse&#39;, &#39;max_depth&#39;: 3, &#39;max_features&#39;: None, &#39;max_leaf_nodes&#39;: None, &#39;min_impurity_decrease&#39;: 0.0, &#39;min_impurity_split&#39;: None, &#39;min_samples_leaf&#39;: 1, &#39;min_samples_split&#39;: 2, &#39;min_weight_fraction_leaf&#39;: 0.0, &#39;presort&#39;: &#39;deprecated&#39;, &#39;random_state&#39;: None, &#39;splitter&#39;: &#39;best&#39;}</code></pre>
<div class="sourceCode" id="cb56"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb56-1"><a href="machine-learning-using-python.html#cb56-1"></a>le.get_params()</span></code></pre></div>
<pre><code>## {}</code></pre>
<blockquote>
<p>Linear regression is entirely determined by the functional form of
the prediction equation,i.e., the “formula” we use. It doesn’t have any parameters to tune per se. Improving a linear regression involves playing
with the different predictors and transforming them to improve the predictions. This involve subjects called <em>regression diagnostics</em> and
<em>feature engineering</em> that we will leave to Google for now.</p>
</blockquote>
<p>We can tune different parameters for the decision tree to try and see if
some combination of parameters can improve predictions. One way to do this,
since we’re using a computer, is a grid search. This means that we can set out sets of values of the parameters we want to tune, and the computer will go through every combination of those values to see how the model
performs, and will provide the “best” model.</p>
<p>We would specify the values as a dictionary to the function <code>GridSearchCV</code>, which would optimize based on the cross-validation error.</p>
<div class="sourceCode" id="cb58"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb58-1"><a href="machine-learning-using-python.html#cb58-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> GridSearchCV</span>
<span id="cb58-2"><a href="machine-learning-using-python.html#cb58-2"></a><span class="im">import</span> numpy.random <span class="im">as</span> rnd</span>
<span id="cb58-3"><a href="machine-learning-using-python.html#cb58-3"></a>rnd.RandomState(<span class="dv">39358</span>)</span></code></pre></div>
<pre><code>## RandomState(MT19937) at 0x138936B40</code></pre>
<div class="sourceCode" id="cb60"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb60-1"><a href="machine-learning-using-python.html#cb60-1"></a>param_grid <span class="op">=</span> {<span class="st">&#39;max_depth&#39;</span>: [<span class="dv">1</span>,<span class="dv">3</span>,<span class="dv">5</span>,<span class="dv">7</span>, <span class="dv">10</span>], <span class="st">&#39;min_samples_leaf&#39;</span>: [<span class="dv">1</span>,<span class="dv">5</span>,<span class="dv">10</span>,<span class="dv">20</span>],</span>
<span id="cb60-2"><a href="machine-learning-using-python.html#cb60-2"></a>  <span class="st">&#39;max_features&#39;</span> : [<span class="st">&#39;auto&#39;</span>,<span class="st">&#39;sqrt&#39;</span>]}</span>
<span id="cb60-3"><a href="machine-learning-using-python.html#cb60-3"></a></span>
<span id="cb60-4"><a href="machine-learning-using-python.html#cb60-4"></a>clf <span class="op">=</span> GridSearchCV(dt, param_grid, scoring <span class="op">=</span> <span class="st">&#39;r2&#39;</span>, cv <span class="op">=</span> <span class="dv">5</span>) <span class="co"># Tuning dt</span></span>
<span id="cb60-5"><a href="machine-learning-using-python.html#cb60-5"></a>clf.fit(X_train, y_train)</span></code></pre></div>
<pre><code>## GridSearchCV(cv=5, estimator=DecisionTreeRegressor(max_depth=3),
##              param_grid={&#39;max_depth&#39;: [1, 3, 5, 7, 10],
##                          &#39;max_features&#39;: [&#39;auto&#39;, &#39;sqrt&#39;],
##                          &#39;min_samples_leaf&#39;: [1, 5, 10, 20]},
##              scoring=&#39;r2&#39;)</code></pre>
<div class="sourceCode" id="cb62"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb62-1"><a href="machine-learning-using-python.html#cb62-1"></a>clf.best_estimator_</span></code></pre></div>
<pre><code>## DecisionTreeRegressor(max_depth=10, max_features=&#39;auto&#39;, min_samples_leaf=5)</code></pre>
<div class="sourceCode" id="cb64"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb64-1"><a href="machine-learning-using-python.html#cb64-1"></a><span class="bu">print</span>(clf.best_score_)</span></code></pre></div>
<pre><code>## 0.9645368138311821</code></pre>
<p>So how does this do on the test set?</p>
<div class="sourceCode" id="cb66"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb66-1"><a href="machine-learning-using-python.html#cb66-1"></a>p <span class="op">=</span> clf.best_estimator_.predict(X_test)</span>
<span id="cb66-2"><a href="machine-learning-using-python.html#cb66-2"></a>r2_score(y_test, p)</span></code></pre></div>
<pre><code>## 0.9660556416016673</code></pre>
<p>So this predictor is doing slightly better on the test set than the training set. This is often an indicator that the model is overfitting on the data. This is probable here, given the extremely high R2 values for this model.</p>
</div>
<div id="feature-selection" class="section level3">
<h3><span class="header-section-number">7.2.6</span> Feature selection</h3>
<p>We can also use cross-validation to do recursive feature selection (or
backwards elimination), based on a predictive score. This is different
from usual stepwise selection methods which are based on a succession of
hypothesis tests.</p>
<div class="sourceCode" id="cb68"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb68-1"><a href="machine-learning-using-python.html#cb68-1"></a><span class="im">from</span> sklearn.feature_selection <span class="im">import</span> RFECV</span>
<span id="cb68-2"><a href="machine-learning-using-python.html#cb68-2"></a></span>
<span id="cb68-3"><a href="machine-learning-using-python.html#cb68-3"></a>selector <span class="op">=</span> RFECV(lm, cv <span class="op">=</span> <span class="dv">5</span>, scoring <span class="op">=</span> <span class="st">&#39;r2&#39;</span>)</span>
<span id="cb68-4"><a href="machine-learning-using-python.html#cb68-4"></a>selector <span class="op">=</span> selector.fit(X_train, y_train)</span>
<span id="cb68-5"><a href="machine-learning-using-python.html#cb68-5"></a>selector.support_</span></code></pre></div>
<pre><code>## array([ True, False, False,  True, False, False,  True, False,  True,
##         True,  True,  True,  True,  True,  True, False,  True,  True,
##         True,  True, False,  True,  True,  True,  True,  True])</code></pre>
<p>The support gives the set of predictors (True) that are finally selected.</p>
<div class="sourceCode" id="cb70"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb70-1"><a href="machine-learning-using-python.html#cb70-1"></a>X_train.columns[selector.support_]</span></code></pre></div>
<pre><code>## Index([&#39;carat&#39;, &#39;x&#39;, &#39;cut_Fair&#39;, &#39;cut_Ideal&#39;, &#39;cut_Premium&#39;, &#39;cut_Very Good&#39;,
##        &#39;color_D&#39;, &#39;color_E&#39;, &#39;color_F&#39;, &#39;color_G&#39;, &#39;color_I&#39;, &#39;color_J&#39;,
##        &#39;clarity_I1&#39;, &#39;clarity_IF&#39;, &#39;clarity_SI2&#39;, &#39;clarity_VS1&#39;, &#39;clarity_VS2&#39;,
##        &#39;clarity_VVS1&#39;, &#39;clarity_VVS2&#39;],
##       dtype=&#39;object&#39;)</code></pre>
<p>This is indicating that the best predictive model for the linear regression includes carat, cut, color and clarity, and width of the stone.</p>
</div>
<div id="logistic-regression-1" class="section level3">
<h3><span class="header-section-number">7.2.7</span> Logistic regression</h3>
<p>We noted that logistic regression is available both through <strong>statsmodels</strong> and through <strong>scikit-learn</strong>. Let’s now try to fit a
logistic regression model using <strong>scikit-learn</strong>. We will use the same
Titanic dataset we used earlier.</p>
<div class="sourceCode" id="cb72"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb72-1"><a href="machine-learning-using-python.html#cb72-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb72-2"><a href="machine-learning-using-python.html#cb72-2"></a><span class="im">import</span> statsmodels.api <span class="im">as</span> sm</span>
<span id="cb72-3"><a href="machine-learning-using-python.html#cb72-3"></a><span class="im">import</span> statsmodels.formula.api <span class="im">as</span> smf</span>
<span id="cb72-4"><a href="machine-learning-using-python.html#cb72-4"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb72-5"><a href="machine-learning-using-python.html#cb72-5"></a></span>
<span id="cb72-6"><a href="machine-learning-using-python.html#cb72-6"></a>titanic <span class="op">=</span> sm.datasets.get_rdataset(<span class="st">&#39;Titanic&#39;</span>,<span class="st">&#39;Stat2Data&#39;</span>).data.dropna()</span>
<span id="cb72-7"><a href="machine-learning-using-python.html#cb72-7"></a>titanic.info()</span></code></pre></div>
<pre><code>## &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
## Int64Index: 756 entries, 0 to 1312
## Data columns (total 6 columns):
##  #   Column    Non-Null Count  Dtype  
## ---  ------    --------------  -----  
##  0   Name      756 non-null    object 
##  1   PClass    756 non-null    object 
##  2   Age       756 non-null    float64
##  3   Sex       756 non-null    object 
##  4   Survived  756 non-null    int64  
##  5   SexCode   756 non-null    int64  
## dtypes: float64(1), int64(2), object(3)
## memory usage: 41.3+ KB</code></pre>
<p>We will model <code>Survived</code> on the age, sex and passenger class of passengers.</p>
<div class="sourceCode" id="cb74"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb74-1"><a href="machine-learning-using-python.html#cb74-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb74-2"><a href="machine-learning-using-python.html#cb74-2"></a></span>
<span id="cb74-3"><a href="machine-learning-using-python.html#cb74-3"></a>X <span class="op">=</span> pd.get_dummies(titanic[[<span class="st">&#39;Age&#39;</span>,<span class="st">&#39;Sex&#39;</span>,<span class="st">&#39;PClass&#39;</span>]], drop_first<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb74-4"><a href="machine-learning-using-python.html#cb74-4"></a>y <span class="op">=</span> titanic.Survived</span>
<span id="cb74-5"><a href="machine-learning-using-python.html#cb74-5"></a></span>
<span id="cb74-6"><a href="machine-learning-using-python.html#cb74-6"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y , test_size <span class="op">=</span> <span class="fl">0.2</span>, random_state<span class="op">=</span> <span class="dv">40</span>) <span class="co"># 80/20 split</span></span>
<span id="cb74-7"><a href="machine-learning-using-python.html#cb74-7"></a></span>
<span id="cb74-8"><a href="machine-learning-using-python.html#cb74-8"></a>lrm <span class="op">=</span> LogisticRegression()</span>
<span id="cb74-9"><a href="machine-learning-using-python.html#cb74-9"></a>lrm.fit(X_train, y_train)</span></code></pre></div>
<pre><code>## LogisticRegression()</code></pre>
<p>There are a few differences that are now evident between this model and
the model we fit using <strong>statsmodels</strong>. As a reminder, we fit this model again below.</p>
<div class="sourceCode" id="cb76"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb76-1"><a href="machine-learning-using-python.html#cb76-1"></a>titanic1 <span class="op">=</span> titanic.loc[X_train.index,:]</span>
<span id="cb76-2"><a href="machine-learning-using-python.html#cb76-2"></a>titanic2 <span class="op">=</span> titanic.loc[X_test.index,:]</span>
<span id="cb76-3"><a href="machine-learning-using-python.html#cb76-3"></a>mod_logistic <span class="op">=</span> smf.glm(<span class="st">&#39;Survived ~ Age + Sex + PClass&#39;</span>, data<span class="op">=</span>titanic1,</span>
<span id="cb76-4"><a href="machine-learning-using-python.html#cb76-4"></a>  family <span class="op">=</span> sm.families.Binomial()).fit()</span>
<span id="cb76-5"><a href="machine-learning-using-python.html#cb76-5"></a>mod_logistic.summary()</span></code></pre></div>
<pre><code>## &lt;class &#39;statsmodels.iolib.summary.Summary&#39;&gt;
## &quot;&quot;&quot;
##                  Generalized Linear Model Regression Results                  
## ==============================================================================
## Dep. Variable:               Survived   No. Observations:                  604
## Model:                            GLM   Df Residuals:                      599
## Model Family:                Binomial   Df Model:                            4
## Link Function:                  logit   Scale:                          1.0000
## Method:                          IRLS   Log-Likelihood:                -282.34
## Date:                Thu, 04 Jun 2020   Deviance:                       564.68
## Time:                        21:30:56   Pearson chi2:                     666.
## No. Iterations:                     5                                         
## Covariance Type:            nonrobust                                         
## =================================================================================
##                     coef    std err          z      P&gt;|z|      [0.025      0.975]
## ---------------------------------------------------------------------------------
## Intercept         3.6795      0.440      8.362      0.000       2.817       4.542
## Sex[T.male]      -2.5138      0.221    -11.353      0.000      -2.948      -2.080
## PClass[T.2nd]    -1.2057      0.290     -4.155      0.000      -1.774      -0.637
## PClass[T.3rd]    -2.5974      0.305     -8.528      0.000      -3.194      -2.000
## Age              -0.0367      0.008     -4.385      0.000      -0.053      -0.020
## =================================================================================
## &quot;&quot;&quot;</code></pre>
<p>We can see the objects that are available to us from the two models using
<code>dir(lrm)</code> and <code>dir(mod_logistic)</code>. We find that <code>lrm</code> does not give us
any parameter estimates, p-values or summary methods. It is much leaner, and, in line with other machine learning models, emphasizes predictions. So if you want to find associations between predictors and outcome, you will have to use the <strong>statsmodels</strong> version.</p>
<p>Let’s compare the predictions.</p>
<div class="sourceCode" id="cb78"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb78-1"><a href="machine-learning-using-python.html#cb78-1"></a>plt.clf()</span>
<span id="cb78-2"><a href="machine-learning-using-python.html#cb78-2"></a>p1 <span class="op">=</span> lrm.predict_proba(X_test)[:,<span class="dv">1</span>]</span>
<span id="cb78-3"><a href="machine-learning-using-python.html#cb78-3"></a>p2 <span class="op">=</span> mod_logistic.predict(titanic2)</span>
<span id="cb78-4"><a href="machine-learning-using-python.html#cb78-4"></a></span>
<span id="cb78-5"><a href="machine-learning-using-python.html#cb78-5"></a>plt.plot(p1, p2, <span class="st">&#39;.&#39;</span>)<span class="op">;</span></span>
<span id="cb78-6"><a href="machine-learning-using-python.html#cb78-6"></a>plt.plot([<span class="dv">0</span>,<span class="dv">1</span>],[<span class="dv">0</span>,<span class="dv">1</span>], <span class="st">&#39;:&#39;</span>)<span class="op">;</span></span>
<span id="cb78-7"><a href="machine-learning-using-python.html#cb78-7"></a>plt.xlabel(<span class="st">&#39;scikit-learn&#39;</span>)<span class="op">;</span></span>
<span id="cb78-8"><a href="machine-learning-using-python.html#cb78-8"></a>plt.ylabel(<span class="st">&#39;statsmodels&#39;</span>)<span class="op">;</span></span>
<span id="cb78-9"><a href="machine-learning-using-python.html#cb78-9"></a>plt.title(<span class="st">&#39;Predictions&#39;</span>)<span class="op">;</span></span>
<span id="cb78-10"><a href="machine-learning-using-python.html#cb78-10"></a>plt.show()</span></code></pre></div>
<p><img src="BIOF085_Manual_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>First note that the prediction functions work a bit differently. For <code>lrm</code> we have to explicitly ask for the probability predictions, whereas those are automatically provided for <code>mod_logistic</code>. We also find that the predictions aren’t exactly the same. This is because <code>lrm</code>, by default, runs a penalized regression using the lasso criteria (L2 norm), rather than the non-penalized version that <code>mod_logistic</code> runs. We can specify no penalty for <code>lrm</code> and can see much closer agreement between the two models.</p>
<div class="sourceCode" id="cb79"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb79-1"><a href="machine-learning-using-python.html#cb79-1"></a>lrm <span class="op">=</span> LogisticRegression(penalty<span class="op">=</span><span class="st">&#39;none&#39;</span>)</span>
<span id="cb79-2"><a href="machine-learning-using-python.html#cb79-2"></a>lrm.fit(X_train, y_train)</span></code></pre></div>
<pre><code>## LogisticRegression(penalty=&#39;none&#39;)</code></pre>
<div class="sourceCode" id="cb81"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb81-1"><a href="machine-learning-using-python.html#cb81-1"></a>p1 <span class="op">=</span> lrm.predict_proba(X_test)[:,<span class="dv">1</span>]</span>
<span id="cb81-2"><a href="machine-learning-using-python.html#cb81-2"></a></span>
<span id="cb81-3"><a href="machine-learning-using-python.html#cb81-3"></a>plt.clf()</span>
<span id="cb81-4"><a href="machine-learning-using-python.html#cb81-4"></a>plt.plot(p1, p2, <span class="st">&#39;.&#39;</span>)<span class="op">;</span></span>
<span id="cb81-5"><a href="machine-learning-using-python.html#cb81-5"></a>plt.plot([<span class="dv">0</span>,<span class="dv">1</span>],[<span class="dv">0</span>,<span class="dv">1</span>], <span class="st">&#39;:&#39;</span>)<span class="op">;</span></span>
<span id="cb81-6"><a href="machine-learning-using-python.html#cb81-6"></a>plt.xlabel(<span class="st">&#39;scikit-learn&#39;</span>)<span class="op">;</span></span>
<span id="cb81-7"><a href="machine-learning-using-python.html#cb81-7"></a>plt.ylabel(<span class="st">&#39;statsmodels&#39;</span>)<span class="op">;</span></span>
<span id="cb81-8"><a href="machine-learning-using-python.html#cb81-8"></a>plt.title(<span class="st">&#39;Predictions&#39;</span>)<span class="op">;</span></span>
<span id="cb81-9"><a href="machine-learning-using-python.html#cb81-9"></a>plt.show()</span></code></pre></div>
<p><img src="BIOF085_Manual_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
</div>
</div>
<div id="unsupervised-learning" class="section level2">
<h2><span class="header-section-number">7.3</span> Unsupervised learning</h2>
<p>Unsupervised learning is a class of machine learning methods where we are just trying to identify patterns in the data without any labels. This is in contrast to <em>supervised learning</em>, which are the modeling methods we have discussed above.</p>
<p>Most unsupervised learning methods fall broadly into a set of algorithms called <em>cluster analysis</em>. <strong>scikit-learn</strong> provides several clustering algorithms.</p>
<p><img src="graphs/cluster_choice.png" /></p>
<p>We will demonstrate the two more popular choices – K-Means and Agglomerative clustering (also known as hierarchical clustering). We will use the classic Fisher’s Iris data for this demonstration.</p>
<div class="sourceCode" id="cb82"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb82-1"><a href="machine-learning-using-python.html#cb82-1"></a><span class="im">import</span> statsmodels.api <span class="im">as</span> sm</span>
<span id="cb82-2"><a href="machine-learning-using-python.html#cb82-2"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb82-3"><a href="machine-learning-using-python.html#cb82-3"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb82-4"><a href="machine-learning-using-python.html#cb82-4"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb82-5"><a href="machine-learning-using-python.html#cb82-5"></a></span>
<span id="cb82-6"><a href="machine-learning-using-python.html#cb82-6"></a><span class="im">from</span> sklearn.cluster <span class="im">import</span> KMeans, AgglomerativeClustering</span>
<span id="cb82-7"><a href="machine-learning-using-python.html#cb82-7"></a></span>
<span id="cb82-8"><a href="machine-learning-using-python.html#cb82-8"></a>iris <span class="op">=</span> sm.datasets.get_rdataset(<span class="st">&#39;iris&#39;</span>).data</span>
<span id="cb82-9"><a href="machine-learning-using-python.html#cb82-9"></a>sns.relplot(data<span class="op">=</span>iris, x <span class="op">=</span> <span class="st">&#39;Sepal.Length&#39;</span>,y <span class="op">=</span> <span class="st">&#39;Sepal.Width&#39;</span>, hue <span class="op">=</span> <span class="st">&#39;Species&#39;</span>)<span class="op">;</span></span></code></pre></div>
<pre><code>## &lt;seaborn.axisgrid.FacetGrid object at 0x127e49ca0&gt;</code></pre>
<p>The K-Means algorithm takes a pre-specified number of clusters as input, and then tries to find contiguous regions of the data to parse into clusters.</p>
<div class="sourceCode" id="cb84"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb84-1"><a href="machine-learning-using-python.html#cb84-1"></a>km <span class="op">=</span> KMeans(n_clusters <span class="op">=</span> <span class="dv">3</span>)</span>
<span id="cb84-2"><a href="machine-learning-using-python.html#cb84-2"></a>km.fit(iris[[<span class="st">&#39;Sepal.Length&#39;</span>,<span class="st">&#39;Sepal.Width&#39;</span>]])<span class="op">;</span></span></code></pre></div>
<pre><code>## KMeans(n_clusters=3)</code></pre>
<div class="sourceCode" id="cb86"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb86-1"><a href="machine-learning-using-python.html#cb86-1"></a>km.labels_</span></code></pre></div>
<pre><code>## array([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
##        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
##        2, 2, 2, 2, 2, 2, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
##        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
##        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
##        0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
##        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1], dtype=int32)</code></pre>
<div class="sourceCode" id="cb88"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb88-1"><a href="machine-learning-using-python.html#cb88-1"></a>iris[<span class="st">&#39;km_labels&#39;</span>] <span class="op">=</span> km.labels_</span>
<span id="cb88-2"><a href="machine-learning-using-python.html#cb88-2"></a>iris[<span class="st">&#39;km_labels&#39;</span>] <span class="op">=</span> iris.km_labels.astype(<span class="st">&#39;category&#39;</span>)</span>
<span id="cb88-3"><a href="machine-learning-using-python.html#cb88-3"></a></span>
<span id="cb88-4"><a href="machine-learning-using-python.html#cb88-4"></a>sns.relplot(data<span class="op">=</span>iris, x <span class="op">=</span> <span class="st">&#39;Sepal.Length&#39;</span>, y <span class="op">=</span> <span class="st">&#39;Sepal.Width&#39;</span>, </span>
<span id="cb88-5"><a href="machine-learning-using-python.html#cb88-5"></a>           hue <span class="op">=</span> <span class="st">&#39;km_labels&#39;</span>)<span class="op">;</span></span></code></pre></div>
<pre><code>## &lt;seaborn.axisgrid.FacetGrid object at 0x13ad45e20&gt;</code></pre>
<p>Agglomerative clustering takes a different approach. It starts by coalescing individual points successively, based on a distance metric and a principle for how to coalesce groups of points (called <em>linkage</em>). The number of clusters can then be determined either visually or via different cutoffs.</p>
<div class="sourceCode" id="cb90"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb90-1"><a href="machine-learning-using-python.html#cb90-1"></a>hc <span class="op">=</span> AgglomerativeClustering(distance_threshold<span class="op">=</span><span class="dv">0</span>, n_clusters<span class="op">=</span><span class="va">None</span>, </span>
<span id="cb90-2"><a href="machine-learning-using-python.html#cb90-2"></a>                             linkage<span class="op">=</span><span class="st">&#39;complete&#39;</span>)</span>
<span id="cb90-3"><a href="machine-learning-using-python.html#cb90-3"></a></span>
<span id="cb90-4"><a href="machine-learning-using-python.html#cb90-4"></a>hc.fit(iris[[<span class="st">&#39;Sepal.Length&#39;</span>,<span class="st">&#39;Sepal.Width&#39;</span>]])</span></code></pre></div>
<pre><code>## AgglomerativeClustering(distance_threshold=0, linkage=&#39;complete&#39;,
##                         n_clusters=None)</code></pre>
<div class="sourceCode" id="cb92"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb92-1"><a href="machine-learning-using-python.html#cb92-1"></a>hc.linkage</span></code></pre></div>
<pre><code>## &#39;complete&#39;</code></pre>
<div class="sourceCode" id="cb94"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb94-1"><a href="machine-learning-using-python.html#cb94-1"></a><span class="im">from</span> scipy.cluster.hierarchy <span class="im">import</span> dendrogram</span>
<span id="cb94-2"><a href="machine-learning-using-python.html#cb94-2"></a></span>
<span id="cb94-3"><a href="machine-learning-using-python.html#cb94-3"></a><span class="co">## The following is from https://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_dendrogram.html</span></span>
<span id="cb94-4"><a href="machine-learning-using-python.html#cb94-4"></a><span class="kw">def</span> plot_dendrogram(model, <span class="op">**</span>kwargs):</span>
<span id="cb94-5"><a href="machine-learning-using-python.html#cb94-5"></a>    <span class="co"># Create linkage matrix and then plot the dendrogram</span></span>
<span id="cb94-6"><a href="machine-learning-using-python.html#cb94-6"></a></span>
<span id="cb94-7"><a href="machine-learning-using-python.html#cb94-7"></a>    <span class="co"># create the counts of samples under each node</span></span>
<span id="cb94-8"><a href="machine-learning-using-python.html#cb94-8"></a>    counts <span class="op">=</span> np.zeros(model.children_.shape[<span class="dv">0</span>])</span>
<span id="cb94-9"><a href="machine-learning-using-python.html#cb94-9"></a>    n_samples <span class="op">=</span> <span class="bu">len</span>(model.labels_)</span>
<span id="cb94-10"><a href="machine-learning-using-python.html#cb94-10"></a>    <span class="cf">for</span> i, merge <span class="kw">in</span> <span class="bu">enumerate</span>(model.children_):</span>
<span id="cb94-11"><a href="machine-learning-using-python.html#cb94-11"></a>        current_count <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb94-12"><a href="machine-learning-using-python.html#cb94-12"></a>        <span class="cf">for</span> child_idx <span class="kw">in</span> merge:</span>
<span id="cb94-13"><a href="machine-learning-using-python.html#cb94-13"></a>            <span class="cf">if</span> child_idx <span class="op">&lt;</span> n_samples:</span>
<span id="cb94-14"><a href="machine-learning-using-python.html#cb94-14"></a>                current_count <span class="op">+=</span> <span class="dv">1</span>  <span class="co"># leaf node</span></span>
<span id="cb94-15"><a href="machine-learning-using-python.html#cb94-15"></a>            <span class="cf">else</span>:</span>
<span id="cb94-16"><a href="machine-learning-using-python.html#cb94-16"></a>                current_count <span class="op">+=</span> counts[child_idx <span class="op">-</span> n_samples]</span>
<span id="cb94-17"><a href="machine-learning-using-python.html#cb94-17"></a>        counts[i] <span class="op">=</span> current_count</span>
<span id="cb94-18"><a href="machine-learning-using-python.html#cb94-18"></a></span>
<span id="cb94-19"><a href="machine-learning-using-python.html#cb94-19"></a>    linkage_matrix <span class="op">=</span> np.column_stack([model.children_, model.distances_,</span>
<span id="cb94-20"><a href="machine-learning-using-python.html#cb94-20"></a>                                      counts]).astype(<span class="bu">float</span>)</span>
<span id="cb94-21"><a href="machine-learning-using-python.html#cb94-21"></a></span>
<span id="cb94-22"><a href="machine-learning-using-python.html#cb94-22"></a>    <span class="co"># Plot the corresponding dendrogram</span></span>
<span id="cb94-23"><a href="machine-learning-using-python.html#cb94-23"></a>    dendrogram(linkage_matrix, <span class="op">**</span>kwargs)</span>
<span id="cb94-24"><a href="machine-learning-using-python.html#cb94-24"></a></span>
<span id="cb94-25"><a href="machine-learning-using-python.html#cb94-25"></a>plot_dendrogram(hc, truncate_mode<span class="op">=</span><span class="st">&#39;level&#39;</span>, p<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb94-26"><a href="machine-learning-using-python.html#cb94-26"></a>plt.xlabel(<span class="st">&quot;Number of points in node (or index of point if no parenthesis).&quot;</span>)</span>
<span id="cb94-27"><a href="machine-learning-using-python.html#cb94-27"></a>plt.show()</span></code></pre></div>
<p><img src="BIOF085_Manual_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<div class="sourceCode" id="cb95"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb95-1"><a href="machine-learning-using-python.html#cb95-1"></a>hc <span class="op">=</span> AgglomerativeClustering( n_clusters<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb95-2"><a href="machine-learning-using-python.html#cb95-2"></a>                             linkage<span class="op">=</span><span class="st">&#39;average&#39;</span>)</span>
<span id="cb95-3"><a href="machine-learning-using-python.html#cb95-3"></a></span>
<span id="cb95-4"><a href="machine-learning-using-python.html#cb95-4"></a>hc.fit(iris[[<span class="st">&#39;Sepal.Length&#39;</span>,<span class="st">&#39;Sepal.Width&#39;</span>]])<span class="op">;</span></span></code></pre></div>
<pre><code>## AgglomerativeClustering(linkage=&#39;average&#39;, n_clusters=3)</code></pre>
<div class="sourceCode" id="cb97"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb97-1"><a href="machine-learning-using-python.html#cb97-1"></a>hc.labels_</span></code></pre></div>
<pre><code>## array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
##        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
##        0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0, 0, 2, 2, 2, 2, 2,
##        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
##        2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 0, 2, 2, 2, 1, 2, 2, 1, 0, 1, 2, 1,
##        2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 1, 2, 2, 1, 2, 2, 2, 1, 1, 1,
##        2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])</code></pre>
<div class="sourceCode" id="cb99"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb99-1"><a href="machine-learning-using-python.html#cb99-1"></a>iris[<span class="st">&#39;hc_labels&#39;</span>] <span class="op">=</span> pd.Series(hc.labels_).astype(<span class="st">&#39;category&#39;</span>)</span>
<span id="cb99-2"><a href="machine-learning-using-python.html#cb99-2"></a></span>
<span id="cb99-3"><a href="machine-learning-using-python.html#cb99-3"></a>sns.relplot(data<span class="op">=</span>iris, x <span class="op">=</span> <span class="st">&#39;Sepal.Length&#39;</span>, y<span class="op">=</span> <span class="st">&#39;Sepal.Width&#39;</span>, </span>
<span id="cb99-4"><a href="machine-learning-using-python.html#cb99-4"></a>           hue <span class="op">=</span> <span class="st">&#39;hc_labels&#39;</span>)<span class="op">;</span></span></code></pre></div>
<pre><code>## &lt;seaborn.axisgrid.FacetGrid object at 0x148c67a30&gt;</code></pre>
<blockquote>
<p>Play around with different linkage methods to see how these clusters change.</p>
</blockquote>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="statistical-analysis.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="string-manipulation.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 1
},
"edit": {
"link": "https://github.com/ARAASTAT/BIOF085/edit/master/book/05_python_learning.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["BIOF085_Manual.pdf", "https://github.com/ARAASTAT/BIOF085/raw/master/book/05_python_learning.Rmd"],
"toc": {
"collapse": "subsection",
"scroll_highlight": true
}
});
});
</script>

</body>

</html>
